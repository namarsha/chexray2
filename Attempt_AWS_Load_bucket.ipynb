{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzipping very large files: https://stackoverflow.com/questions/339053/how-do-you-unzip-very-large-files-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: The below code extracts the Stanford Chexpert Dataset, which must be downloaded through the Stanford ML group website.\n",
    "\n",
    "To run the code below, ensure the Chexpert-V1.0-small.zip file is in the same directory as this notebook. Note that the file made by this code is very large. Use with caution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join('.', 'input', 'pulmonary-chest-xray-abnormalities\\\\')\n",
    "\n",
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the Montgomery set files\n",
    "\n",
    "mont_dir = \".\\\\input\\\\Montgomery\\\\MontgomerySet\\\\\"\n",
    "mont_paths = []\n",
    "for filename in glob.iglob(mont_dir + \"**/*\", recursive=True):\n",
    "     mont_paths.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the China set files\n",
    "\n",
    "shen_dir = \".\\\\input\\\\ChinaSet_AllFiles\\\\ChinaSet_AllFiles\\\\\"\n",
    "shen_paths = []\n",
    "for filename in glob.iglob(shen_dir + \"**/*\", recursive=True):\n",
    "     shen_paths.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Montgomery Files', len(mont_paths))\n",
    "print('Shenzhen Files', len(shen_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now combine al the files into a dataframe: all_paths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_paths_df = pd.DataFrame(dict(path = mont_paths + shen_paths))\n",
    "all_paths_df['source'] = all_paths_df['path'].map(lambda x: x.split('\\\\')[3])\n",
    "all_paths_df['file_id'] = all_paths_df['path'].map(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "all_paths_df['patient_group']  = all_paths_df['file_id'].map(lambda x: x.split('_')[0])\n",
    "\n",
    "all_paths_df['file_ext'] = all_paths_df['path'].map(lambda x: os.path.splitext(x)[1][1:])\n",
    "all_paths_df = all_paths_df[all_paths_df.file_ext.isin(['png', 'txt'])]\n",
    "all_paths_df['pulm_state']  = all_paths_df['file_id'].map(lambda x: int(x.split('_')[-1]))\n",
    "all_paths_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Report DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_patients_df = all_paths_df.pivot_table(index = ['patient_group', 'pulm_state', 'file_id'], \n",
    "                                             columns=['file_ext'], \n",
    "                                             values = 'path', aggfunc='first').reset_index()\n",
    "clean_patients_df.sample(5)\n",
    "from warnings import warn\n",
    "def report_to_dict(in_path):\n",
    "    with open(in_path, 'r') as f:\n",
    "        all_lines = [x.strip() for x in f.read().split('\\n')]\n",
    "    info_dict = {}\n",
    "    try:\n",
    "        if \"Patient's Sex\" in all_lines[0]:\n",
    "            info_dict['age'] = all_lines[1].split(':')[-1].strip().replace('Y', '')\n",
    "            info_dict['gender'] = all_lines[0].split(':')[-1].strip()\n",
    "            info_dict['report'] = ' '.join(all_lines[2:]).strip()\n",
    "        else:\n",
    "            info_dict['age'] = all_lines[0].split(' ')[-1].replace('yrs', '').replace('yr', '')\n",
    "            info_dict['gender'] = all_lines[0].split(' ')[0].strip()\n",
    "            info_dict['report'] = ' '.join(all_lines[1:]).strip()\n",
    "        \n",
    "        info_dict['gender'] = info_dict['gender'].upper().replace('FEMALE', 'F').replace('MALE', 'M').replace('FEMAL', 'F')[0:1]\n",
    "        if 'month' in info_dict.get('age', ''):\n",
    "            info_dict.pop('age') # invalid\n",
    "        if 'day' in info_dict.get('age', ''):\n",
    "            info_dict.pop('age') # invalid\n",
    "        elif len(info_dict.get('age',''))>0:\n",
    "            info_dict['age'] = float(info_dict['age'])\n",
    "        else:\n",
    "            info_dict.pop('age')\n",
    "        return info_dict\n",
    "    except Exception as e:\n",
    "        print(all_lines)\n",
    "        warn(str(e), RuntimeWarning)\n",
    "        return {}\n",
    "report_df = pd.DataFrame([dict(**report_to_dict(c_row.pop('txt')), **c_row) \n",
    "              for  _, c_row in clean_patients_df.iterrows()])\n",
    "report_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the mask files\n",
    "#mask_path = os.path.join(\"D:\\\\\", \"Documents\", \"Medical\", \"TB\",\"Lung Segmentation\",\"masks\")\n",
    "mask_path = os.path.join('.', 'input', 'masks')\n",
    "#mask directory\n",
    "masks = os.listdir(mask_path)\n",
    "\n",
    "\n",
    "#clean it up to align with images names: Remove the .png and the _mask(from China masks)\n",
    "\n",
    "\n",
    "mask_ids_temp = [fName.split(\".png\")[0] for fName in masks]\n",
    "\n",
    "mask_ids = [fName.split(\"_mask\")[0] for fName in mask_ids_temp]\n",
    "\n",
    "#The total # of masks\n",
    "mask_file_names = [os.path.join(mask_path, mask) for mask in masks]\n",
    "\n",
    "#masks\n",
    "\n",
    "#Total number of modified masks - China masks\n",
    "check = [i for i in masks if \"mask\" in i]\n",
    "print(\"Total mask that has modified name:\",len(check))\n",
    "\n",
    "## ??? There seems to be 704 masks before modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the image files\n",
    "image_path = os.path.join('.', 'input',\"CXR_png\")\n",
    "\n",
    "#image directory\n",
    "images = os.listdir(image_path)\n",
    "\n",
    "#clean it up to align with images names: Remove the .png and the _mask(from China masks)\n",
    "image_ids = [fName.split(\".png\")[0] for fName in images]\n",
    "#mask_file_names = [fName.split(\"_mask\")[0] for fName in mask_id]\n",
    "\n",
    "image_file_names = [os.path.join(image_path, image) for image in images]\n",
    "\n",
    "#The total # of images\n",
    "print('Total X-ray images: ', len(image_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put all the names into a dataframe for convenience\n",
    "images_df = pd.DataFrame()\n",
    "images_df['xrays'] = image_file_names\n",
    "images_df['file_id'] = image_ids\n",
    "images_df['has_mask'] = images_df['file_id'].isin(mask_ids)\n",
    "\n",
    "images_with_masks_df = images_df[images_df['file_id'].isin(mask_ids)]\n",
    "\n",
    "images_with_masks_df['masks'] = mask_file_names\n",
    "\n",
    "print(\"There are {} x-rays with masks\".format(len(images_with_masks_df)))\n",
    "images_df\n",
    "print(\"True indicates the x-ray has a mask:\")\n",
    "images_df['has_mask'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Do a train-test split\n",
    "#??? So, here we are splitting the xrays from the masks, in segmentation we are trying to predict the mask.\n",
    "# We use 90% of the data for the training set.\n",
    "train_x,test_x,train_y,test_y = train_test_split(images_with_masks_df['xrays'],\n",
    "                                                   images_with_masks_df['masks'],test_size    = 0.1,\n",
    "                                                   random_state = 42)\n",
    "\n",
    "#size of the training set should be 90% of 704\n",
    "#len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are also going to make a validation set.\n",
    "trainx,validationx,trainy,validationy = train_test_split(train_x,train_y,test_size = 0.1,random_state = 42)\n",
    "\n",
    "#len(trainx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put all these data sets into data frames\n",
    "train_df = pd.DataFrame(index=trainx.index)\n",
    "train_df['xrays'] = trainx\n",
    "train_df['masks'] = trainy\n",
    "\n",
    "test_df = pd.DataFrame(index=test_x.index)\n",
    "test_df['xrays'] = test_x\n",
    "test_df['masks'] = test_y\n",
    "\n",
    "validation_df = pd.DataFrame(index=validationx.index)\n",
    "validation_df['xrays'] = validationx\n",
    "validation_df['masks'] = validationy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that we have a dataframe of training and test examples, can we mask them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need a train info dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info_loc = os.path.join(\".\", \"CheXpert-v1.0-small\") #Need the file path to the CheXpert-V1.0-small file (this must be downloaded independently through Stanford ML)\n",
    "train_file_name = \"train.csv\"\n",
    "train_info = pd.read_csv(os.path.join(train_info_loc, train_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info.fillna(0, inplace=True)\n",
    "train_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a new dataframe with a column for complete path and diagnostic columns of interest:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(index=train_info.index)\n",
    "data_df = train_info.iloc[:, 5:].copy()\n",
    "data_df['xrays'] = [os.path.join('.', x) for x in train_info['Path'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_df.to_pickle(\"train_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get and view file from the data_df (checking understanding of file formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_xray = data_df['xrays'][354]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(rnd_xray) #Note, these .jpg files are PIL objects...\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's convert it to a tensor\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "convert_tensor = transforms.ToTensor()\n",
    "\n",
    "img_t = convert_tensor(img)\n",
    "\n",
    "\n",
    "print(img_t.shape)\n",
    "\n",
    "shifted = img_t.permute(1, 2, 0)\n",
    "\n",
    "print(shifted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create test train split\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Initialize the GroupShuffleSplit.\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.01)\n",
    "\n",
    "# Get the indexers for the split.\n",
    "idx1, idx2 = next(gss.split(data_df, groups=data_df.index))\n",
    "\n",
    "# Get the split DataFrames.\n",
    "df1, df2 = data_df.iloc[idx1], data_df.iloc[idx2]\n",
    "\n",
    "\n",
    "\n",
    "#Just use a slice of the images for now:\n",
    "\n",
    "train_temp_df = df1.sample(100000)\n",
    "test_temp_df = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp_df['xrays'][432]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now is the time to regroup and think about what you are doing. What do you need to accomplish and how will you get there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "1) Load 1 Resnet pretrained model\n",
    "2) Apply this model to data (What's the input and what's the output?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "## Try implementing a Resnet from scratch  (tutorial here: https://www.youtube.com/watch?v=DkNIBBBvcPs)\n",
    "\n",
    "\n",
    "class resblock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample = None, stride = 1):\n",
    "        super(resblock, self).__init__()\n",
    "        self.expansion = 4 # \"number of channels after a block is 4x what it was when it entered\"\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride =1, padding = 0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = stride, padding =1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels *self.expansion, kernel_size =1, stride=1, padding = 0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "            \n",
    "            print('SHAPES:')\n",
    "            print(x.shape)\n",
    "            print(identity.shape)\n",
    "        \n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ResNet(nn.Module): # note, the layers argument corresponds to the number of resnet blocks\n",
    "    def __init__(self, resblock, layers, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n",
    "        \n",
    "        #ResNet layers\n",
    "        \n",
    "        self.layer1 = self._make_layer(resblock, layers[0], out_channels=64, stride=1)\n",
    "        self.layer2 = self._make_layer(resblock, layers[1], out_channels=128, stride=2)\n",
    "        self.layer3 = self._make_layer(resblock, layers[2], out_channels=256, stride=2)\n",
    "        self.layer4 = self._make_layer(resblock, layers[3], out_channels=512, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512*4, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        print(x.shape)\n",
    "        x = self.maxpool(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def _make_layer(self, resblock, num_residual_blocks, out_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "        \n",
    "        if stride != 1 or self.in_channels != out_channels * 4:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels *4, kernel_size = 1,\n",
    "                                                         stride = stride),\n",
    "                                               nn.BatchNorm2d(out_channels*4))\n",
    "        \n",
    "        layers.append(resblock(self.in_channels, out_channels, identity_downsample, stride)) #changes the number of channels\n",
    "        self.in_channels = out_channels * 4\n",
    "        \n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(resblock(self.in_channels, out_channels))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize resnet 50 with our parameters, 1 channel for grayscale images, 14 classes.\n",
    "\n",
    "def ResNet50(img_channels=1, num_classes=14):\n",
    "    return ResNet(resblock, [3, 4, 6, 3], img_channels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net = ResNet50()\n",
    "    x = torch.randn(2, 1, 224, 224)\n",
    "    y = net(x).to('cuda')\n",
    "    print(y.shape)\n",
    "    \n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
